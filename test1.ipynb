{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import ast\n",
    "import torch.optim as optim\n",
    "from performer_pytorch import PerformerEncDec\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "\n",
    "#NUM_BATCHES = int(1e5)\n",
    "NUM_BATCHES = 10\n",
    "# BATCH_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "GENERATE_EVERY  = 100\n",
    "# NUM_TOKENS = 16 + 2\n",
    "NUM_TOKENS = 28996 + 2\n",
    "# ENC_SEQ_LEN = 32\n",
    "# DEC_SEQ_LEN = 64 + 1\n",
    "ENC_SEQ_LEN = 512\n",
    "DEC_SEQ_LEN = 256 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "def cycle():\n",
    "    while True:\n",
    "        prefix = torch.ones((BATCH_SIZE, 1)).long().cuda()\n",
    "        src = torch.randint(2, NUM_TOKENS, (BATCH_SIZE, ENC_SEQ_LEN)).long().cuda()\n",
    "        tgt = torch.cat((prefix, src, src), 1)\n",
    "        src_mask = torch.ones(BATCH_SIZE, ENC_SEQ_LEN).bool().cuda()\n",
    "        tgt_mask = torch.ones(BATCH_SIZE, tgt.shape[1]).bool().cuda()\n",
    "        yield (src, tgt, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "    # def __init__(self):\n",
    "\n",
    "        summary_data = pandas.read_csv(filename)\n",
    "        # summary_data = pandas.read_csv(\"/home/ayan/data/python_files/my_summ_data/datasets/train_tokens_sample.csv\")\n",
    "        \n",
    "        x = summary_data['src_txt_tokens'].apply(ast.literal_eval)\n",
    "        y = summary_data['tgt_txt_tokens'].apply(ast.literal_eval)\n",
    "        xm = summary_data['src_txt_att_mask'].apply(ast.literal_eval)\n",
    "        ym = summary_data['tgt_txt_att_mask'].apply(ast.literal_eval)\n",
    "\n",
    "        \n",
    "        # self.X = torch.tensor(list(zip(*itertools.zip_longest(*x, fillvalue = 0))))\n",
    "        # self.Y = torch.tensor(list(zip(*itertools.zip_longest(*y, fillvalue = 0))))\n",
    "        # self.X_mask = torch.tensor(list(zip(*itertools.zip_longest(*x_att, fillvalue = 0))))\n",
    "        # self.Y_mask = torch.tensor(list(zip(*itertools.zip_longest(*y_att, fillvalue = 0))))\n",
    "        \n",
    "        self.X = torch.tensor(x)\n",
    "        self.Y = torch.tensor(y)\n",
    "        self.X_mask = torch.tensor(xm)\n",
    "        self.Y_mask = torch.tensor(ym)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        src = self.X[index]\n",
    "        src_msk = self.X_mask[index].bool()\n",
    "        \n",
    "        one = torch.ones(1)\n",
    "        # tgt = torch.cat((one, self.Y[index], self.Y[index]), 0)\n",
    "        # tgt_msk = torch.cat((one, self.Y_mask[index], self.Y_mask[index]), 0).bool()\n",
    "        tgt = torch.cat((one, self.Y[index]), 0)\n",
    "        tgt_msk = torch.cat((one, self.Y_mask[index]), 0).bool()\n",
    "        \n",
    "        # return (src[0:32], src_msk[0:32], tgt[0:32], tgt_msk[0:32])\n",
    "        return (src, src_msk, tgt, tgt_msk, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data_csv = pandas.read_csv(\"/home/ayan/ayan_fed_home/data/python_files/my_summ_data/datasets/train_tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data_h5 = pandas.read_hdf(\"/home/ayan/ayan_fed_home/data/python_files/my_summ_data/datasets/train_tokens.h5\", \"/samples/samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dataset = SummaryDataset(\"/home/ayan/ayan_fed_home/data/python_files/my_summ_data/datasets/train_tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dataloader = DataLoader(summary_dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "\n",
    "model = PerformerEncDec(\n",
    "    dim=512,\n",
    "    enc_num_tokens=NUM_TOKENS,\n",
    "    enc_depth=1,\n",
    "    enc_heads=8,\n",
    "    enc_max_seq_len=ENC_SEQ_LEN,\n",
    "    enc_reversible=True,\n",
    "    enc_feature_redraw_interval=1000,\n",
    "    enc_nb_features = 64,\n",
    "    dec_num_tokens=NUM_TOKENS,\n",
    "    dec_depth=3,\n",
    "    dec_heads=8,\n",
    "    dec_max_seq_len=DEC_SEQ_LEN,\n",
    "    dec_reversible=True,\n",
    "    dec_feature_redraw_interval=1000,\n",
    "    dec_nb_features=64\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in tqdm.tqdm(range(NUM_BATCHES), mininterval = 10., desc = 'training'):\n",
    "rng = math.ceil(287083/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(rng), mininterval = 10., desc = 'training'):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    src, src_mask, tgt, tgt_mask = next(iter(summary_dataloader))\n",
    "    src_mask = src_mask.cuda()\n",
    "    src = src.long().cuda()\n",
    "    tgt_mask = tgt_mask.cuda()\n",
    "    tgt = tgt.long().cuda()\n",
    "    \n",
    "    with autocast():\n",
    "        loss = model(src, tgt, enc_mask = src_mask, dec_mask = tgt_mask)\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "       \n",
    "    print(f'{i}: {loss.item()}')\n",
    "\n",
    "    scaler.step(optim)\n",
    "    scaler.update()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i != 0 and i % GENERATE_EVERY == 0:\n",
    "        \n",
    "        model.eval()\n",
    "        src, src_mask, _, _ = next(iter(summary_dataloader))\n",
    "\n",
    "        src, src_mask = src[:1], src_mask[:1]\n",
    "        start_tokens = (torch.ones((1, 1)) * 1).long().cuda()\n",
    "\n",
    "        src = src.cuda()\n",
    "        src_mask = src_mask.cuda()\n",
    "\n",
    "        sample = model.generate(src, start_tokens, ENC_SEQ_LEN, enc_mask=src_mask)\n",
    "        incorrects = (src != sample).abs().sum()\n",
    "\n",
    "        print(f\"input:  \", src)\n",
    "        print(f\"predicted output:  \", sample)\n",
    "        print(f\"incorrects: {incorrects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(rng), mininterval = 10., desc = 'training'):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    #src, tgt, src_mask, tgt_mask = next(cycle())\n",
    "    src, src_mask, tgt, tgt_mask = next(iter(summary_dataloader))\n",
    "    # print(type(src))\n",
    "    # print(type(src_mask))\n",
    "    # print(type(tgt))\n",
    "    # print(type(tgt_mask))\n",
    "    \n",
    "    src_mask = src_mask.cuda()\n",
    "    src = src.long().cuda()\n",
    "    tgt_mask = tgt_mask.cuda()\n",
    "    tgt = tgt.long().cuda()\n",
    "    \n",
    "    # src_mask = src_mask.cuda(0)\n",
    "    # src = src.long().cuda(0)\n",
    "    # tgt_mask = tgt_mask.cuda(0)\n",
    "    # tgt = tgt.long().cuda(0)\n",
    "    \n",
    "    # print (src.shape, src_mask.shape, tgt.shape, tgt_mask.shape)\n",
    "    #model = model.cuda()\n",
    "    \n",
    "\n",
    "    with autocast():\n",
    "        loss = model(src, tgt, enc_mask = src_mask, dec_mask = tgt_mask)\n",
    "        #loss = model(src, tgt)\n",
    "    \n",
    "    # print('lossp1 - ', loss)\n",
    "    # print('lossp1 - ', type(loss))\n",
    "    # print('lossp1 - ', loss.shape)\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    \n",
    "    print(f'{i}: {loss.item()}')\n",
    "\n",
    "    scaler.step(optim)\n",
    "    scaler.update()\n",
    "    optim.zero_grad()\n",
    "    # break\n",
    "\n",
    "    if i != 0 and i % GENERATE_EVERY == 0:\n",
    "        \n",
    "        model.eval()\n",
    "        #src, _, src_mask, _ = next(cycle())\n",
    "        src, src_mask, _, _ = next(iter(summary_dataloader))\n",
    "\n",
    "        src, src_mask = src[:1], src_mask[:1]\n",
    "        start_tokens = (torch.ones((1, 1)) * 1).long().cuda()\n",
    "\n",
    "        sample = model.generate(src, start_tokens, ENC_SEQ_LEN, enc_mask=src_mask)\n",
    "        incorrects = (src != sample).abs().sum()\n",
    "\n",
    "        print(f\"input:  \", src)\n",
    "        print(f\"predicted output:  \", sample)\n",
    "        print(f\"incorrects: {incorrects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data_t = pandas.read_csv(\"/home/ayan/ayan_fed_home/data/python_files/my_summ_data/datasets/train_tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "GENERATE_EVERY  = 100\n",
    "NUM_TOKENS = 28996 + 2\n",
    "ENC_SEQ_LEN = 512\n",
    "DEC_SEQ_LEN = 256 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n"
     ]
    }
   ],
   "source": [
    "model = PerformerEncDec(\n",
    "    dim=512,\n",
    "    enc_num_tokens=NUM_TOKENS,\n",
    "    enc_depth=1,\n",
    "    enc_heads=8,\n",
    "    enc_max_seq_len=ENC_SEQ_LEN,\n",
    "    enc_reversible=True,\n",
    "    enc_feature_redraw_interval=1000,\n",
    "    enc_nb_features = 64,\n",
    "    dec_num_tokens=NUM_TOKENS,\n",
    "    dec_depth=3,\n",
    "    dec_heads=8,\n",
    "    dec_max_seq_len=DEC_SEQ_LEN,\n",
    "    dec_reversible=True,\n",
    "    dec_feature_redraw_interval=1000,\n",
    "    dec_nb_features=64\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# src, src_mask, tgt, tgt_mask, indices = next(iter(summary_dataloader))\n",
    "\n",
    "def single_run(src, src_mask, tgt, tgt_mask):\n",
    "\n",
    "    src = src.cuda()\n",
    "    src_mask = src_mask.bool().cuda()\n",
    "    tgt = tgt.cuda()\n",
    "    tgt_mask = tgt_mask.bool().cuda()\n",
    "\n",
    "    with autocast():\n",
    "        loss = model(src, tgt, enc_mask = src_mask, dec_mask = tgt_mask)\n",
    "\n",
    "    print(src.shape, src_mask.shape, tgt.shape, tgt_mask.shape)\n",
    "    #print(indices)\n",
    "    print(loss)\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    # print(f'{i}: {loss.item()}')\n",
    "    print(f'loss: {loss.item()}')\n",
    "\n",
    "    scaler.step(optim)\n",
    "    scaler.update()\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 512]) torch.Size([64, 512]) torch.Size([64, 257]) torch.Size([64, 257])\n",
      "tensor(10.3939, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "loss: 10.393864631652832\n"
     ]
    }
   ],
   "source": [
    "samples = summary_data_t.iloc[[103819, 142689,  13439, 112329,  32677, 275564,  90784,  18786,  91488,\n",
    "        195170, 249460, 228088,  16303,  37961, 204093, 115905, 192251, 179068,\n",
    "        196260, 136432, 164435, 158048, 255970,   5525, 272237, 246354,  51773,\n",
    "        105490, 178813, 130813, 229303, 163523, 212595, 220619, 242735, 269087,\n",
    "         58262, 277290, 221507, 172436, 140651, 237822,  53877, 252209,  57175,\n",
    "         41408, 273560, 124275, 278838, 109448, 247685, 235827,  18839, 122117,\n",
    "        272651, 223672,   1371, 108826,  43890, 262758,  52505, 251462, 232237,\n",
    "          7040]]\n",
    "\n",
    "x = torch.tensor(list(samples['src_txt_tokens'].apply(ast.literal_eval)))\n",
    "xm = torch.tensor(list(samples['src_txt_att_mask'].apply(ast.literal_eval)))\n",
    "\n",
    "y = list(samples['tgt_txt_tokens'].apply(ast.literal_eval))\n",
    "ym = list(samples['tgt_txt_att_mask'].apply(ast.literal_eval))\n",
    "\n",
    "for i in range(len(y)):\n",
    "  y[i] = [1] + y[i]\n",
    "  ym[i] = [1] + ym[i]\n",
    "\n",
    "y = torch.tensor(y)\n",
    "ym = torch.tensor(ym)\n",
    "\n",
    "single_run(x, xm, y, ym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 101,\n",
       " 1488,\n",
       " 1465,\n",
       " 1177,\n",
       " 18778,\n",
       " 4164,\n",
       " 1766,\n",
       " 2474,\n",
       " 1120,\n",
       " 6927,\n",
       " 1633,\n",
       " 1107,\n",
       " 2388,\n",
       " 1107,\n",
       " 2223,\n",
       " 1884,\n",
       " 118,\n",
       " 5048,\n",
       " 1705,\n",
       " 133,\n",
       " 186,\n",
       " 135,\n",
       " 1131,\n",
       " 2373,\n",
       " 1482,\n",
       " 112,\n",
       " 188,\n",
       " 18046,\n",
       " 1106,\n",
       " 11778,\n",
       " 1123,\n",
       " 4035,\n",
       " 23655,\n",
       " 2737,\n",
       " 2269,\n",
       " 4196,\n",
       " 133,\n",
       " 186,\n",
       " 135,\n",
       " 1131,\n",
       " 2536,\n",
       " 170,\n",
       " 23609,\n",
       " 7340,\n",
       " 1186,\n",
       " 187,\n",
       " 10658,\n",
       " 2168,\n",
       " 1372,\n",
       " 1105,\n",
       " 17200,\n",
       " 1111,\n",
       " 2495,\n",
       " 20064,\n",
       " 1116,\n",
       " 133,\n",
       " 186,\n",
       " 135,\n",
       " 1118,\n",
       " 1103,\n",
       " 1159,\n",
       " 1131,\n",
       " 1355,\n",
       " 1106,\n",
       " 11078,\n",
       " 1513,\n",
       " 117,\n",
       " 1131,\n",
       " 1108,\n",
       " 9588,\n",
       " 1105,\n",
       " 25194,\n",
       " 117,\n",
       " 26562,\n",
       " 19120,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[179310, 129559, 266582,  29970, 123514, 138101,  29450,  28677,  66329,\n",
    "        172908,  86640,  19724, 221796, 260803, 268974,  52179,  54780, 149421,\n",
    "          3628,  44860, 154409, 116264,  65573, 138291,  44087,  78587, 177545,\n",
    "        202400,  67990, 250929, 139273,  88409,  69672, 168532,   3151, 104263,\n",
    "        199137,   8649, 160722, 119363, 162033, 126911, 280892, 273120, 266279,\n",
    "        173412,  56441,  28284,  12484, 103112,  42305, 211312, 219730, 196961,\n",
    "        145970, 280487, 284682, 167988,  74578, 216525, 128626,  14263, 279115,\n",
    "        221270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = summary_data_t['src_txt_tokens'].apply(ast.literal_eval)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
